\section{Introduction}
Web browsing is one of the most widely-used applications used in today's Internet ecosystem. Measuring and understanding the performance of web browsing is, therefore, of the utmost importance. A number of metrics have been developed and used as benchmarks to accurately reflect the performance of web applications for users. The diversity of web pages, user devices, browsers, metrics (e.g. browser-centric and user-centric) as well as the lack of clearly established standards lead to difficulties when quantifying performance.

A number of studies (e.g. \citet{10.1007/978-3-319-30505-9_17}) over the years have attempted to measure web performance in some form. Studies that perform research on web performance are required to provide adequate details regarding chosen metrics, tools, and data sources to ensure that results are correctly interpreted, comparable, and reproducible. \citeauthor{10.1007/978-3-030-15986-3_19} began with a survey of numerous studies on different aspects of web (performance) and summarized the different tools and metrics used. Although one third of the surveyed studies did not precisely specific metrics and/or data sources, the most commonly used tools were identified and incorporated into subsequent measurements. The survey also gathered and ranked the most important metrics as well as their possible data sources. The tools, metrics, and data sources identified were then used in the rest of the paper.

\citeauthor{10.1007/978-3-030-15986-3_19} continue by creating a test environment in which different tools and metrics, researched during the aforementioned survey, can be compared with one another. The tools that were used in the majority of the surveyed papers were utilized: Firefox with Selenium, Firefox with Marionette, and Chrome with DevTools. The paper continues by comparing the differences in metrics as well as results and outlining the most prominent pitfalls that should be avoided when measuring web performance. These pitfalls include the inclusion of HTTP redirects in timing statistics and the choice of correct measurements for object sizes and counts. The paper concludes with a set of guidelines for web performance studies to improve reproducibility and ensure the correct interpretation of data. This paper examines the original measurements as well as new measurements using modern versions of the aforementioned tools to examine the validity of the recommendations / pitfalls in the original paper.