\section{Conclusion}
In this paper, we re-implemented the paper Web Performance Pitfalls, published in 2019 during the International Conference for Passive and Active Measurement (PAM). The re-implementation focused on reproducing the conclusions of the original paper using the data provided publicly as well as taking new measurements to confirm it. During the implementation process, problems with a number of the provided scripts caused smaller adjustments to be necessary and no measurements to be taken with 2/3 of the frameworks (i.e. Firefox with Selenium and Chrome with DevTools). Three new datasets were created using the scripts, based on the Alexa 1000 list.

This paper's reproduction effort focused mainly on three main pitfalls that were described in the original paper: Redirects, Object Size, and Object Count. The results required for the conclusions drawn for these pitfalls were confirmed by the original data, but only 2 / 3 were confirmed by the new measurements created using Firefox with Marionette on a modern system. The new dataset contradicts the conclusion in the original paper that the Content-Length in HAR files is the most reliable data source for object sizes but does confirm the large inconsistencies in object size between data sources. The other examined conclusions regarding redirects and object count / byte index were confirmed by the new dataset.

The plots and results found in this work and the original paper can be reproduced using the data and scripts available online, as described in Section \ref{sec:reimplement}. The data in both papers underlines the importance of improving the documentation of studies of web performance as well as choosing performance metrics carefully and deliberately.